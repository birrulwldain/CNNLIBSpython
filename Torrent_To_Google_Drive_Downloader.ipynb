{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Torrent To Google Drive Downloader",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/birrulwldain/CNNLIBSpython/blob/main/Torrent_To_Google_Drive_Downloader.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 1: Instal pustaka yang diperlukan\n",
        "!pip install h5py scipy numpy\n",
        "\n",
        "# Langkah 2: Impor pustaka\n",
        "import numpy as np\n",
        "import h5py\n",
        "import os\n",
        "from scipy.signal import find_peaks\n",
        "from google.colab import drive, files\n",
        "\n",
        "# Langkah 3: Mount Google Drive untuk menyimpan hasil\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Langkah 4: Unggah file pure_spectra.h5\n",
        "print(\"Silakan unggah file pure_spectra.h5\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Pastikan file telah diunggah\n",
        "h5_path = \"/content/pure_spectra.h5\"\n",
        "if not os.path.exists(h5_path):\n",
        "    raise FileNotFoundError(\"File pure_spectra.h5 tidak ditemukan! Pastikan Anda mengunggah file tersebut.\")\n",
        "\n",
        "# Langkah 5: Buat direktori untuk menyimpan hasil augmentasi di Google Drive\n",
        "output_dir = \"/content/gdrive/My Drive/libs_lstm/data/processed/augmented\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Fungsi untuk memuat spektrum\n",
        "def load_spectra(h5_path):\n",
        "    \"\"\"\n",
        "    Load pure spectra from HDF5 file for Ca I only.\n",
        "    Returns: List of (spectra, labels, wavelengths, element, ion, temp) tuples.\n",
        "    \"\"\"\n",
        "    print(f\"Opening HDF5 file: {h5_path}\")\n",
        "    data = []\n",
        "    with h5py.File(h5_path, \"r\") as f:\n",
        "        expected_groups = 5  # Hanya 5 spektrum untuk Ca I\n",
        "        for group in f.keys():\n",
        "            try:\n",
        "                parts = group.split(\"_\")\n",
        "                if len(parts) != 3:\n",
        "                    print(f\"Skipping {group}: Invalid format\")\n",
        "                    continue\n",
        "                element, ion, temp = parts\n",
        "                ion = int(ion)\n",
        "                temp = int(float(temp.replace(\"K\", \"\")))\n",
        "\n",
        "                # Hanya ambil data untuk Ca I\n",
        "                if element != \"Ca\" or ion != 1:\n",
        "                    continue\n",
        "\n",
        "                spectrum_group = f[group][\"spectrum\"]\n",
        "                if \"block0_values\" not in spectrum_group:\n",
        "                    print(f\"Skipping {group}: No block0_values in spectrum\")\n",
        "                    continue\n",
        "                spectrum_data = spectrum_group[\"block0_values\"][:]\n",
        "                intensity = spectrum_data[:, 1]\n",
        "                intensity = (intensity - intensity.min()) / (intensity.max() - intensity.min())\n",
        "                wavelengths = spectrum_data[:, 0]\n",
        "\n",
        "                labels_group = f[group][\"labels\"]\n",
        "                if \"block1_values\" not in labels_group:\n",
        "                    print(f\"Skipping {group}: No block1_values in labels\")\n",
        "                    continue\n",
        "                labels = labels_group[\"block1_values\"][:].flatten()\n",
        "\n",
        "                data.append((intensity, labels, wavelengths, element, ion, temp))\n",
        "                print(f\"Loaded group {group}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {group}: {e}\")\n",
        "                continue\n",
        "        print(f\"Total groups loaded: {len(data)}/{expected_groups}\")\n",
        "        if len(data) != expected_groups:\n",
        "            print(f\"Warning: Expected {expected_groups} groups, got {len(data)}\")\n",
        "        if not data:\n",
        "            raise ValueError(\"No valid data loaded from HDF5 file\")\n",
        "    return data\n",
        "\n",
        "# Fungsi untuk menerapkan efek Doppler dan Stark\n",
        "def apply_doppler_stark_broadening(spectrum, wavelengths, temp, ne_scale_range=(1e15, 1e18), temp_variation=0.05, add_baseline=True, add_noise=True, add_intensity_scale=True):\n",
        "    \"\"\"\n",
        "    Apply Doppler and Stark broadening with additional noise.\n",
        "    Parameters:\n",
        "    - spectrum: Original spectrum intensity.\n",
        "    - wavelengths: Wavelength array (in nm).\n",
        "    - temp: Plasma temperature (in K).\n",
        "    - ne_scale_range: Tuple (min, max) for electron density (in cm^-3).\n",
        "    - temp_variation: Fractional variation in temperature (± fraction).\n",
        "    - add_baseline: If True, add a small polynomial baseline.\n",
        "    - add_noise: If True, add Gaussian noise.\n",
        "    - add_intensity_scale: If True, add intensity scaling.\n",
        "    \"\"\"\n",
        "    # 1. Variasikan suhu dan kerapatan elektron untuk simulasi\n",
        "    temp_var = np.random.uniform(1 - temp_variation, 1 + temp_variation)\n",
        "    temp_adjusted = temp * temp_var\n",
        "    ne = np.random.uniform(ne_scale_range[0], ne_scale_range[1])  # Kerapatan elektron (cm^-3)\n",
        "\n",
        "    # 2. Deteksi puncak emisi dengan parameter yang lebih sensitif\n",
        "    peaks, _ = find_peaks(spectrum, height=0.02, distance=5, prominence=0.01)  # Deteksi puncak kecil\n",
        "    if len(peaks) == 0:\n",
        "        print(\"Warning: No peaks detected in spectrum!\")\n",
        "        return spectrum\n",
        "\n",
        "    # 3. Hitung lebar Doppler (Gaussian)\n",
        "    c = 3e8  # Kecepatan cahaya (m/s)\n",
        "    k = 1.38e-23  # Konstanta Boltzmann (J/K)\n",
        "    m = 6.64e-26  # Massa atom Ca (kg)\n",
        "    lambda_0 = wavelengths[peaks]  # Panjang gelombang puncak (nm)\n",
        "    delta_lambda_d = (lambda_0 * 1e-9 / c) * np.sqrt(8 * k * temp_adjusted * np.log(2) / m) * 1e9  # Konversi ke nm\n",
        "\n",
        "    # 4. Hitung lebar Stark (Lorentzian)\n",
        "    w = 0.005  # Parameter Stark untuk Ca I\n",
        "    delta_lambda_s = w * (ne / 1e16)  # Lebar Stark (nm)\n",
        "\n",
        "    # 5. Kombinasi lebar (sederhana: gunakan Lorentzian dengan lebar efektif)\n",
        "    delta_lambda = delta_lambda_d + delta_lambda_s  # Aproksimasi lebar efektif\n",
        "\n",
        "    # 6. Buat spektrum baru dengan pelebaran garis\n",
        "    synthetic_spectrum = np.zeros_like(spectrum)\n",
        "    dwl = np.mean(np.diff(wavelengths))  # Resolusi panjang gelombang\n",
        "    for i, peak_idx in enumerate(peaks):\n",
        "        # Profil Lorentzian untuk pelebaran\n",
        "        sigma = delta_lambda[i] / 2  # Lebar Lorentzian (FWHM -> sigma)\n",
        "        wl_center = wavelengths[peak_idx]\n",
        "        intensity = spectrum[peak_idx]\n",
        "        lorentzian = intensity * (sigma**2 / ((wavelengths - wl_center)**2 + sigma**2))\n",
        "        synthetic_spectrum += lorentzian\n",
        "\n",
        "    # 7. Tambahkan baseline kecil (opsional)\n",
        "    if add_baseline:\n",
        "        x = np.linspace(-1, 1, len(spectrum))\n",
        "        baseline_scale = 0.0001  # Baseline kecil (~0.01)\n",
        "        a = np.random.uniform(-baseline_scale, baseline_scale)\n",
        "        b = np.random.uniform(-baseline_scale/2, baseline_scale/2)\n",
        "        c = np.random.uniform(0, baseline_scale/2)\n",
        "        baseline = a * x**2 + b * x + c\n",
        "        synthetic_spectrum = synthetic_spectrum + baseline\n",
        "\n",
        "    # 8. Tambahkan skala intensitas (opsional)\n",
        "    if add_intensity_scale:\n",
        "        intensity_scale = np.random.uniform(0.8, 1.0)  # Variasi ±20%\n",
        "        synthetic_spectrum = synthetic_spectrum * intensity_scale\n",
        "\n",
        "    # 9. Tambahkan noise Gaussian (opsional)\n",
        "    if add_noise:\n",
        "        noise_scale = 0.0001  # 1% dari intensitas maksimum\n",
        "        noise = np.random.normal(0, noise_scale, spectrum.shape)\n",
        "        synthetic_spectrum = synthetic_spectrum + noise\n",
        "\n",
        "    # 10. Normalisasi ulang spektrum\n",
        "    synthetic_spectrum = (synthetic_spectrum - synthetic_spectrum.min()) / (synthetic_spectrum.max() - synthetic_spectrum.min())\n",
        "    synthetic_spectrum = np.clip(synthetic_spectrum, 0, 1)\n",
        "    return synthetic_spectrum\n",
        "\n",
        "# Fungsi untuk menghasilkan label\n",
        "def generate_labels(spectrum, temp):\n",
        "    \"\"\"\n",
        "    Generate binary labels with adaptive threshold based on temperature.\n",
        "    \"\"\"\n",
        "    base_threshold = np.median(spectrum) + 2 * np.std(spectrum)\n",
        "    temp_factor = temp / 6000  # Skala berdasarkan suhu (normalisasi terhadap suhu terendah)\n",
        "    threshold = base_threshold * temp_factor\n",
        "    return (spectrum > threshold).astype(np.int32)\n",
        "\n",
        "# Fungsi untuk augmentasi data\n",
        "def augment_with_doppler_stark(data, output_dir, element, ion, split, global_counters, temp_step=500):\n",
        "    \"\"\"\n",
        "    Augment spectra for Ca I using Doppler and Stark broadening with 500K temperature steps and additional noise.\n",
        "    \"\"\"\n",
        "    print(f\"Starting augmentation for {element} {ion} ({split} split)...\")\n",
        "    print(f\"Number of spectra loaded: {len(data)}\")\n",
        "    if len(data) == 0:\n",
        "        raise ValueError(f\"No data to augment for {element} {ion}!\")\n",
        "\n",
        "    # Tentukan suhu baru dengan langkah 500 K\n",
        "    min_temp = 6000\n",
        "    max_temp = 30000\n",
        "    temp_range = np.arange(min_temp, max_temp + temp_step, temp_step)\n",
        "    num_temps = len(temp_range)  # Total suhu (49 untuk 6000K hingga 30000K dengan langkah 500K)\n",
        "    total_spectra = 2000  # Total spektrum yang diinginkan\n",
        "    spectra_per_temp = total_spectra // num_temps  # ~40 spektrum per suhu\n",
        "    print(f\"Generating spectra for {num_temps} temperatures ({min_temp}K to {max_temp}K) with {spectra_per_temp} spectra per temperature\")\n",
        "\n",
        "    output_h5 = os.path.join(output_dir, f\"{split}.h5\")\n",
        "    split_size = int(spectra_per_temp * num_temps * {\"train\": 0.7, \"validation\": 0.15, \"test\": 0.15}[split])\n",
        "    print(f\"Target size for {split} split: {split_size}\")\n",
        "\n",
        "    # Buka file H5 dalam mode append\n",
        "    with h5py.File(output_h5, \"a\") as f_out:\n",
        "        count = 0\n",
        "        for target_temp in temp_range:\n",
        "            # Temukan spektrum asli terdekat untuk digunakan sebagai template\n",
        "            closest_data = min(data, key=lambda x: abs(x[5] - target_temp))\n",
        "            intensity, orig_labels, wavelengths, element, ion, _ = closest_data\n",
        "            print(f\"Generating spectra for {element}_{ion}_{target_temp}K using template from {closest_data[5]}K\")\n",
        "\n",
        "            for aug_idx in range(spectra_per_temp):\n",
        "                if count >= split_size:\n",
        "                    break\n",
        "                # Terapkan pelebaran Doppler dan Stark dengan suhu target\n",
        "                synthetic_spectrum = apply_doppler_stark_broadening(\n",
        "                    intensity,\n",
        "                    wavelengths,\n",
        "                    target_temp,  # Gunakan suhu target\n",
        "                    ne_scale_range=(1e15, 1e18),  # Rentang lebih besar\n",
        "                    temp_variation=0.05,\n",
        "                    add_baseline=True,\n",
        "                    add_noise=True,\n",
        "                    add_intensity_scale=True\n",
        "                )\n",
        "\n",
        "                synthetic_labels = generate_labels(synthetic_spectrum, target_temp)\n",
        "\n",
        "                # Gunakan global counter untuk nama grup unik\n",
        "                global_count = global_counters[split]\n",
        "                group_name = f\"{element}_{ion}_{target_temp}K_{global_count}\"\n",
        "                g = f_out.create_group(group_name)\n",
        "                g.create_dataset(\"spectrum\", data=synthetic_spectrum)\n",
        "                g.create_dataset(\"labels\", data=synthetic_labels)\n",
        "                g.create_dataset(\"wavelengths\", data=wavelengths)\n",
        "                g.attrs[\"element\"] = element\n",
        "                g.attrs[\"ion\"] = ion\n",
        "                g.attrs[\"temp\"] = target_temp\n",
        "\n",
        "                print(f\"Created group {group_name} in {split}.h5\")\n",
        "                global_counters[split] += 1\n",
        "                count += 1\n",
        "            if count >= split_size:\n",
        "                break\n",
        "\n",
        "    print(f\"Added {count} spectra for {element} {ion} to {split}.h5\")\n",
        "\n",
        "# Main execution\n",
        "print(\"Loading spectra...\")\n",
        "data = load_spectra(h5_path)\n",
        "\n",
        "# Hanya untuk Ca I\n",
        "element_ion_pairs = [(\"Ca\", 1)]  # Hanya Ca I\n",
        "\n",
        "# Inisialisasi global counters untuk setiap split\n",
        "global_counters = {\"train\": 0, \"validation\": 0, \"test\": 0}\n",
        "\n",
        "# Hapus file H5 lama jika ada\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    output_h5 = os.path.join(output_dir, f\"{split}.h5\")\n",
        "    if os.path.exists(output_h5):\n",
        "        os.remove(output_h5)\n",
        "        print(f\"Removed existing {split}.h5\")\n",
        "\n",
        "# Augmentasi untuk Ca I saja dengan langkah suhu 500K\n",
        "for element, ion in element_ion_pairs:\n",
        "    print(f\"\\n=== Augmenting for {element} {ion} ===\")\n",
        "    data_subset = [d for d in data if d[3] == element and d[4] == ion]\n",
        "    print(f\"Number of spectra for {element} {ion}: {len(data_subset)}\")\n",
        "    for split in [\"train\", \"validation\", \"test\"]:\n",
        "        augment_with_doppler_stark(data_subset, output_dir, element, ion, split, global_counters, temp_step=500)\n",
        "\n",
        "# Langkah 6: Verifikasi bahwa file telah disimpan\n",
        "print(\"\\nVerifikasi file yang dihasilkan:\")\n",
        "for split in [\"train\", \"validation\", \"test\"]:\n",
        "    output_h5 = os.path.join(output_dir, f\"{split}.h5\")\n",
        "    if os.path.exists(output_h5):\n",
        "        print(f\"{split}.h5 berhasil disimpan di {output_h5}\")\n",
        "    else:\n",
        "        print(f\"WARNING: {split}.h5 tidak ditemukan!\")"
      ],
      "metadata": {
        "id": "wsnu21CSez8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Langkah 1: Instal pustaka yang diperlukan\n",
        "!pip install tensorflow tensorflow-addons h5py numpy matplotlib sklearn\n",
        "\n",
        "# Langkah 2: Impor pustaka\n",
        "import numpy as np\n",
        "import h5py\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.metrics import Precision, Recall\n",
        "import tensorflow_addons as tfa\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from google.colab import drive, files\n",
        "import os\n",
        "\n",
        "# Langkah 3: Mount Google Drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Langkah 4: Tentukan path ke dataset\n",
        "base_path = \"/content/gdrive/My Drive/libs_lstm/data/processed/augmented\"\n",
        "train_h5_path = f\"{base_path}/train.h5\"\n",
        "val_h5_path = f\"{base_path}/validation.h5\"\n",
        "test_h5_path = f\"{base_path}/test.h5\"\n",
        "\n",
        "# Pastikan file ada\n",
        "for path in [train_h5_path, val_h5_path, test_h5_path]:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"File {path} tidak ditemukan! Pastikan file ada di Google Drive atau unggah secara manual.\")\n",
        "\n",
        "# Langkah 5: Fungsi untuk memuat dataset\n",
        "def load_dataset(h5_path):\n",
        "    with h5py.File(h5_path, \"r\") as f:\n",
        "        spectra = []\n",
        "        labels = []\n",
        "        for group in f.keys():\n",
        "            spectrum = f[group][\"spectrum\"][:]\n",
        "            label = f[group][\"labels\"][:]\n",
        "            spectra.append(spectrum)\n",
        "            labels.append(label)\n",
        "    return np.array(spectra), np.array(labels)\n",
        "\n",
        "# Langkah 6: Muat dataset\n",
        "print(\"Memuat dataset...\")\n",
        "X_train, y_train = load_dataset(train_h5_path)\n",
        "X_val, y_val = load_dataset(val_h5_path)\n",
        "X_test, y_test = load_dataset(test_h5_path)\n",
        "\n",
        "# Langkah 7: Reshape untuk LSTM\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_val = X_val.reshape(X_val.shape[0], X_val.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Langkah 8: Hitung bobot kelas untuk menangani ketidakseimbangan label\n",
        "y_train_flat = y_train.flatten()\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.array([0, 1]), y=y_train_flat)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "print(\"Class weights:\", class_weight_dict)\n",
        "\n",
        "# Langkah 9: Buat model LSTM\n",
        "model = Sequential([\n",
        "    LSTM(128, input_shape=(X_train.shape[1], 1), return_sequences=True),\n",
        "    Dropout(0.2),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(64, activation=\"relu\"),\n",
        "    Dense(y_train.shape[1], activation=\"sigmoid\")\n",
        "])\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.0001),\n",
        "    loss=\"binary_crossentropy\",\n",
        "    metrics=[\"accuracy\", Precision(), Recall(), tfa.metrics.F1Score(num_classes=2, average=\"macro\")]\n",
        ")\n",
        "\n",
        "# Langkah 10: Latih model\n",
        "print(\"Melatih model...\")\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    class_weight=class_weight_dict,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Langkah 11: Evaluasi model\n",
        "test_metrics = model.evaluate(X_test, y_test, return_dict=True)\n",
        "print(\"Test Metrics:\", test_metrics)\n",
        "\n",
        "# Langkah 12: Plot training history\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history[\"accuracy\"], label=\"Train Accuracy\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Validation Accuracy\")\n",
        "plt.title(\"Training and Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plot_path = \"/content/gdrive/My Drive/libs_lstm/plots/lstm_training_history_accuracy.png\"\n",
        "os.makedirs(os.path.dirname(plot_path), exist_ok=True)\n",
        "plt.savefig(plot_path)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(history.history[\"f1_score\"], label=\"Train F1-Score\")\n",
        "plt.plot(history.history[\"val_f1_score\"], label=\"Validation F1-Score\")\n",
        "plt.title(\"Training and Validation F1-Score\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.legend()\n",
        "plot_path = \"/content/gdrive/My Drive/libs_lstm/plots/lstm_training_history_f1.png\"\n",
        "plt.savefig(plot_path)\n",
        "plt.show()\n",
        "\n",
        "# Langkah 13: Simpan model\n",
        "model_path = \"/content/gdrive/My Drive/libs_lstm/models/lstm_model_advanced.h5\"\n",
        "os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
        "model.save(model_path)\n",
        "print(f\"Model disimpan di {model_path}\")"
      ],
      "metadata": {
        "id": "OHaFXc19e3rv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}